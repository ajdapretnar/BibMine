{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run storage.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scopus_search(query):\n",
    "    cursor = \"*\"\n",
    "    entries = []\n",
    "    while cursor:\n",
    "        response = requests.get(\"https://api.elsevier.com/content/search/scopus?query=TITLE-ABS-KEY({})&apiKey={}&cursor={}&count=200\"\n",
    "                                .format(query, ELSEVIER_APIKey, cursor))\n",
    "        cont = json.loads(response.content)\n",
    "        if cont[\"search-results\"][\"cursor\"][\"@next\"] != cursor:\n",
    "            cursor = cont[\"search-results\"][\"cursor\"][\"@next\"].replace('+','%2B').replace('/', '%2F')\n",
    "        else:\n",
    "            cursor = None\n",
    "        if \"entry\" not in cont[\"search-results\"]:\n",
    "            break\n",
    "        for e in cont[\"search-results\"][\"entry\"]:\n",
    "            entries.append(e)\n",
    "        print('{}/{}'.format(len(entries), cont[\"search-results\"]['opensearch:totalResults']))\n",
    "    return entries\n",
    "\n",
    "\n",
    "def scopus_get_article(aid, url):\n",
    "    filename = os.path.join('xml', aid + '.xml')\n",
    "    article = storage_download(filename)\n",
    "    if article is None:\n",
    "        print('Fetching content from scopus for: {}'.format(aid))\n",
    "        article = requests.get(\"{}?apiKey={}\".format(url, ELSEVIER_APIKey)).text\n",
    "        storage_upload(article, filename)\n",
    "    return article\n",
    "\n",
    "\n",
    "def scopus_get_json(aid, url):\n",
    "    filename = os.path.join('json', aid + '.json')\n",
    "    data = storage_download(filename)\n",
    "    if data is None:\n",
    "        article = scopus_get_article(aid, url)\n",
    "        entry = scopus_parse_entry(article)\n",
    "        entry['BibMine-url'] = url\n",
    "        entry['BibMine-aid'] = aid\n",
    "        data = json.dumps(entry, sort_keys=True, indent=2, separators=(',', ': '))\n",
    "        storage_upload(data, filename)\n",
    "    entry = json.loads(data)\n",
    "    return entry\n",
    "\n",
    "\n",
    "def get_text(s, t):\n",
    "    h = s.find(t)\n",
    "    if h:\n",
    "        return h.get_text()\n",
    "    return ''\n",
    "\n",
    "def get_full_text(doi):\n",
    "    response = requests.get(\"https://api.elsevier.com/content/article/doi/{}&apiKey={}&httpAccept=text%2Fxml\"\n",
    "                            .format(doi, ELSEVIER_APIKey))\n",
    "    if response.status_code == 404:\n",
    "        return ''\n",
    "    soup = Soup(response.content, features=\"lxml\")\n",
    "    return get_text(soup, 'ce:sections')\n",
    "\n",
    "\n",
    "def scopus_parse_entry(article):\n",
    "    soup = Soup(article, features=\"lxml\")\n",
    "    e = {}\n",
    "    e['Title'] = get_text(soup, 'dc:title').replace('\\n', ' ').replace('\\t', ' ')\n",
    "    e['PublicationName'] = get_text(soup, 'prism:publicationname')\n",
    "    e['Type'] = get_text(soup, 'prism:aggregationtype')\n",
    "    e['Subtype'] = get_text(soup, 'subtypedescription')\n",
    "    if soup.find('ce:para') and soup.find('ce:para').parent.name == 'abstract':\n",
    "        e['Abstract'] = get_text(soup, 'ce:para').replace('\\n', ' ').replace('\\t', ' ')\n",
    "    else:\n",
    "        e['Abstract'] = ''\n",
    "    e['Volume'] = get_text(soup, 'prism:volume')\n",
    "    e['Issue'] = get_text(soup, 'prism:issueidentifier')\n",
    "    e['Page Range'] = get_text(soup, 'prism:pagerange')\n",
    "    e['Date'] = get_text(soup, 'prism:coverdate')\n",
    "    e['EID'] = get_text(soup, 'eid')\n",
    "    e['DOI'] = get_text(soup, 'prism:doi')\n",
    "    e['Content'] = get_full_text(e['DOI'])\n",
    "    e['URL'] = 'https://doi.org/{}'.format(e['DOI'])\n",
    "\n",
    "    # provided by authors\n",
    "    e['Keywords'] = [keyword.get_text() for keyword in soup.find_all('author-keyword')\n",
    "                     if keyword.parent.name == 'authkeywords']\n",
    "    # provided by indexing service\n",
    "    e['IndexTerms'] = [indexterm.get_text() for indexterm in soup.find_all('mainterm')\n",
    "                       if indexterm.parent.name == 'idxterms']\n",
    "    e['OpenAccess'] = get_text(soup, 'openaccess')\n",
    "    if soup.find_all('ref-fulltext'):\n",
    "        e['References'] = [ref.get_text().replace('\\t', ' ') for ref in soup.find_all('ref-fulltext')]\n",
    "    else:\n",
    "        e['References'] = []\n",
    "    if soup.find_all('subject-area'):\n",
    "        e['SubjectAreas'] = [sarea.get_text().replace('\\t', ' ') for sarea in soup.find_all('subject-area')\n",
    "                             if sarea.parent.name == 'subject-areas']\n",
    "    else:\n",
    "        e['SubjectAreas'] = []\n",
    "    e['CitedBy'] = get_text(soup, 'citedby-count')\n",
    "    if soup.find(rel='scopus-citedby'):\n",
    "        e['Scopus Cited By'] = soup.find(rel='scopus-citedby')['href']\n",
    "    else:\n",
    "        e['Scopus Cited By'] = ''\n",
    "    e['Funding Acronym'] = get_text(soup, 'fund-acr')\n",
    "    e['Funding Agency ID'] = get_text(soup, 'fund-no')\n",
    "    e['Funding Agency'] = get_text(soup, 'fund-sponsor')\n",
    "    e['Scopus Identifier'] = get_text(soup, 'dc:identifier')\n",
    "\n",
    "    # authors information\n",
    "    # sometimes authors is blank, and names are listed in <bibrecord><author-group>...</author-group></bibrecord>\n",
    "    e['Authors'] = []\n",
    "    if soup.find('authors'):\n",
    "        authors = soup.find('authors').find_all('author')\n",
    "        for author in authors:\n",
    "            e['Authors'].append({\n",
    "                'seq': author['seq'],\n",
    "                'ce:indexed-name': get_text(author, 'ce:indexed-name'),\n",
    "                'ce:given-name': get_text(author, 'ce:given-name'),\n",
    "                'ce:surname': get_text(author, 'ce:surname'),\n",
    "                'author-url': get_text(author, 'author-url'),\n",
    "                'auid': author['auid'],\n",
    "                'afid': [aff['id'] for aff in author.find_all('affiliation')],\n",
    "            })\n",
    "    # affiliation information\n",
    "    e['Affiliations'] = []\n",
    "    if soup.find('affiliation'):\n",
    "        for aff in soup.find_all('affiliation'):\n",
    "            if aff.parent.name == 'abstracts-retrieval-response':\n",
    "                e['Affiliations'].append({\n",
    "                    'afid': aff['id'],\n",
    "                    'affilname': get_text(aff, 'affilname'),\n",
    "                    'affiliation-city': get_text(aff, 'affiliation-city'),\n",
    "                    'affiliation-country': get_text(aff, 'affiliation-country'),\n",
    "                })\n",
    "\n",
    "    return e\n",
    "\n",
    "\n",
    "def flatten_for_pandas(d):\n",
    "    nd = {}\n",
    "    for k, vl in d.items():\n",
    "        nvl = []\n",
    "        for v in vl:\n",
    "            if type(v) == list:\n",
    "                v = \"; \".join([str(e) for e in v])\n",
    "            else:\n",
    "                v = str(v)\n",
    "            nvl.append(v)\n",
    "        nd[k] = nvl\n",
    "    return nd\n",
    "\n",
    "\n",
    "def scopus_retrive(scopus_query, save_to_file=None):\n",
    "    # get list of article ids\n",
    "    entries = scopus_search(scopus_query)\n",
    "    \n",
    "    # download each article\n",
    "    data = {}\n",
    "    for i, e in enumerate(entries):\n",
    "        if i % 200 == 0:\n",
    "            print(i)\n",
    "        url = e['prism:url']\n",
    "        aid = url.split('/')[-1]\n",
    "\n",
    "        e_data = scopus_get_json(aid, url)\n",
    "\n",
    "        for k, v in e_data.items():\n",
    "            data.setdefault(k, []).append(v)\n",
    "    \n",
    "    # convert to pandas and save\n",
    "    data = flatten_for_pandas(data)\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    if save_to_file:\n",
    "        df.to_csv(save_to_file, index=False)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
