{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from nltk import bigrams\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your own path to the corpus.\n",
    "df = pd.read_csv('../Innovation/Innovation-Scopus-has_abstract.tab', delimiter='\\t')\n",
    "df = df.drop([0, 1])\n",
    "new_df = df[['Title', 'Date', 'Abstract', 'CitedBy']]\n",
    "# remove instances with empty abstracts\n",
    "new_df = new_df.dropna(subset=['Abstract'])\n",
    "pickle.dump(new_df, open('topic-corpus.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "regex = \"(?u)\\\\b[\\\\w-]+\\\\b\"\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    tokenizer = RegexpTokenizer(regex)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    mwe_tokenizer = MWETokenizer([('u', 's', 'a'), ('u', 's'), ('b', 'b')], separator='')\n",
    "    tokens = mwe_tokenizer.tokenize(tokens)\n",
    "    tokens = [t for t in tokens if t not in stop_words and not re.match(\"[0-9]\", t)]\n",
    "    return tokens\n",
    "\n",
    "new_df['tokens'] = tokens = new_df['Abstract'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    return [wordnet_lemmatizer.lemmatize(t) for t in text]\n",
    "\n",
    "new_df['tokens'] = new_df['tokens'].apply(lemmatize)\n",
    "tokens = [t for t in new_df['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"finding\", \"research\", \"purpose\", \"study\", \"methodology\", \"result\", \n",
    "             \"analysis\", \"method\", \"paper\", \"literature\", \"tourism\", \"tourist\", \"innovation\",\n",
    "             \"also\", \"within\", \"whereas\", \"would\"]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return [[token for token in doc if token not in stopwords] for doc in text]\n",
    "\n",
    "new_df['tokens'] = new_df['tokens'].apply(remove_stopwords)\n",
    "tokens = remove_stopwords(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram_stopwords = [\"result show\", \"descriptive statistical\", \"analysis cluster\", \"per cent\",\n",
    "#                    \"goal paper\", \"presented paper\", \"paper present\", \"study examines\", \"study investigates\"]\n",
    "\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_documents(tokens)\n",
    "finder.apply_freq_filter(10)\n",
    "lexicon = [i + \" \" + j for i, j in finder.nbest(bigram_measures.pmi, 100)]\n",
    "\n",
    "def bgrams(text):\n",
    "    for doc in text:\n",
    "        doc.extend([i + \" \" + j for i, j in list(bigrams(doc)) if i + \" \" + j in lexicon])\n",
    "    return text\n",
    "\n",
    "new_df['tokens'] = new_df['tokens'].apply(bgrams)\n",
    "tokens = bgrams(tokens)\n",
    "pickle.dump(tokens, open('tokens.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequencies\n",
    "\n",
    "Find 5 most frequent words in the corpus. Change fdist.most_common to adjust the number of words to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "flat_tokens = [t for doc in tokens for t in doc]\n",
    "\n",
    "fdist = FreqDist(flat_tokens)\n",
    "fdist.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context of words\n",
    "\n",
    "Find common contexts where the words from the list frequently occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Text\n",
    "\n",
    "text = Text(flat_tokens)\n",
    "text.common_contexts(['methodology'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find commonly co-occurring words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
