{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now provide the Elsevier API key and the query. API key can be found [here](https://dev.elsevier.com/apikey/manage). \n",
    "\n",
    "Query is constructed as described in the Elsevier instructions.\n",
    "Boolean logic applies. Use AND to find both occurrences of the keywords, OR to find either occurrence and NOT to exclude a keyword. You can find detailed information [here](https://service.elsevier.com/app/answers/detail/a_id/25974/supporthub/sciencedirect/).\n",
    "\n",
    "Example:\n",
    "\n",
    "*(“market share” OR “leisure tourism”) AND stakeholders AND NOT space*\n",
    "\n",
    "will return articles that contain keywords \"market share stakeholders\" or \"leisure toursim stakeholders\", but never the keyword \"space\". Please note that you need to use quotation marks to browse by phrase, e.g. \"tourism research\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APIKey = \"\"\n",
    "query = \"tourism+AND+innovation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Title': [],\n",
    "        'Authors': [],\n",
    "        'PublicationName': [],\n",
    "        'Type': [],\n",
    "        'Abstract': [],\n",
    "        'Content': [],\n",
    "        'Volume': [],\n",
    "        'Issue': [],\n",
    "        'Date': [],\n",
    "        'Pages': [],\n",
    "        'PII': [],\n",
    "        'Keywords' : [],\n",
    "        'URL' : [],\n",
    "        'OpenAccess': [],\n",
    "        'References': [],\n",
    "        'CitedBy': [],\n",
    "        'AuthorAUID': [],\n",
    "        'AuthKeywords': [],\n",
    "        'SubjectAreas': []\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we retrieve 6000 articles from Elsevier and parse them on the fly. We use Scopus to get 'cited by' information and keywords.\n",
    "\n",
    "We also print progress as we go (at every 25th instance checked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 6000, 25):\n",
    "    print(\"Retrieving {}/6000.\".format(i))\n",
    "    response = requests.get(\n",
    "        \"https://api.elsevier.com/content/search/sciencedirect?query={}&apiKey={}&start={}\".format(query, APIKey, i))\n",
    "    cont = json.loads(response.content)\n",
    "    for j in cont[\"search-results\"][\"entry\"]:\n",
    "        url = j['prism:url']\n",
    "        data['URL'].append(str(url))\n",
    "        article = requests.get(\"{}?apiKey={}\".format(url, APIKey))\n",
    "        soup = Soup(article.content, features=\"lxml\")\n",
    "        \n",
    "        data['Title'].append(soup.find('dc:title').get_text().replace('\\n', ' ') if soup.find('dc:title') else '')\n",
    "        authors = soup.find_all('dc:creator')\n",
    "        data['Authors'].append('; '.join([item.get_text() for item in authors]) if authors else '')\n",
    "        data['PublicationName'].append(soup.find('prism:publicationname').get_text() if \n",
    "                                        soup.find('prism:publicationname') else '')\n",
    "        data['Type'].append(soup.find('prism:aggregationtype').get_text() if\n",
    "                            soup.find('prism:aggregationtype') else '')\n",
    "        data['Abstract'].append(soup.find('dc:description').get_text().replace('\\n', ' ') \n",
    "                                if soup.find('dc:description') else '')\n",
    "        data['Content'].append(soup.find('ce:sections').get_text().replace('\\n', ' ') \n",
    "                               if soup.find('ce:sections') else '')\n",
    "        data['Volume'].append(soup.find('prism:volume').get_text() if soup.find('prism:volume') else '')\n",
    "        data['Issue'].append(soup.find('prism:issueidentifier').get_text() \n",
    "                             if soup.find('prism:issueidentifier') else '')\n",
    "        data['Date'].append(soup.find('prism:coverdate').get_text() if soup.find('prism:coverdate') else '')\n",
    "        data['Pages'].append(soup.find('prism:pagerange').get_text() if soup.find('prism:pagerange') else '')\n",
    "        data['PII'].append(soup.find('pii').get_text() if soup.find('pii') else '')\n",
    "        data['Keywords'].append('; '.join([res.get_text() for res in soup.find_all('dcterms:subject')] \n",
    "                                          if soup.find_all('dcterms:subject') else ''))\n",
    "        data['OpenAccess'].append(soup.find('openaccess').get_text())\n",
    "        data['References'].append('; '.join([label.get_text() for label in soup.find_all('ce:label') \n",
    "                                             if label.parent.name == 'ce:bib-reference']) \n",
    "                                  if soup.find_all('ce:label') else '')\n",
    "        \n",
    "        sc_id = soup.find('scopus-id')\n",
    "        \n",
    "        if sc_id:\n",
    "            scopus = requests.get(\"https://api.elsevier.com/content/abstract/scopus_id/{}?apiKey={}\".format(sc_id.get_text(), APIKey))\n",
    "            soup2 = Soup(scopus.content, features=\"lxml\")\n",
    "\n",
    "            data['CitedBy'].append(soup.find('citedby-count').get_text() if soup.find('citedby-count') else '')\n",
    "            data['AuthorAUID'].append('; '.join([auth['auid'] for auth in soup.find_all('author') \n",
    "                                                 if auth.parent.name == 'authors']) if soup.find_all('author') else '')\n",
    "            data['AuthKeywords'].append('; '.join([label.get_text() for label in soup.find_all('author-keyword') \n",
    "                                                 if label.parent.name == 'authkeywords']) \n",
    "                                      if soup.find_all('author-keyword') else '')\n",
    "            data['SubjectAreas'].append('; '.join([label.get_text() for label in soup.find_all('subject-area') \n",
    "                                                 if label.parent.name == 'subject-areas']) \n",
    "                                      if soup.find_all('subject-area') else '')\n",
    "        else:\n",
    "            data['CitedBy'].append('')\n",
    "            data['AuthorAUID'].append('')\n",
    "            data['AuthKeywords'].append('')\n",
    "            data['SubjectAreas'].append('')\n",
    "        \n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    df.to_csv(os.path.join(os.getcwd(), 'Innovation/Innovation_{}.csv'.format(i)), index=False)\n",
    "    data = data.fromkeys(data, [])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
