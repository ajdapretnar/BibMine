{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "We will build a topic model with Latent Dirichlet Allocation method. First, we will prepare our data with tokenization and preprocessing. Then, we will build a model and display its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare our stopword list for subsequent filtering. We use nltk's stopword list for English and add our corpus-specific terms to it. Add your own words to the list as suggested by the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords', quiet=True)\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "# add additional stopwords that are relevant for the corpus\n",
    "en_stop.update(['tourism', 'tourist', 'innovation', 'research', 'study',\n",
    "                'paper'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for tokenization. We use regex tokenizer that uses words only. Punctuation will be discarded. Also note that this method will split words like _let's_ into _let_ and _s_ and omit special signs, such as #. It will keep the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # tokenize by words only, e.g. \"The cat chased its tail.\" outputs [\"The\", \"cat\", \"chased\", \"its\", \"tail\"]\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = [w.lower() for s in sent_tokenize(text) for w in tokenizer.tokenize(s)]\n",
    "    return [token for token in tokens if token not in en_stop and len(token)> 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the file that we wish to use as corpus. Replace the file with your own if you wish to use another data set instead of Innovation.csv.\n",
    "\n",
    "col defines the name of the column that will be used for analysis. Here we use 'Title' for speed, but you can also use 'Content' or 'Abstract'. As a matter of fact, you can use any column from data (printed in the section below), but it has to contain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'Authors', 'PublicationName', 'Type', 'Abstract', 'Content', 'Volume', 'Issue', 'Date', 'Pages', 'PII', 'Keywords', 'URL', 'OpenAccess', 'References', 'CitedBy', 'AuthorAUID', 'AuthKeywords', 'SubjectAreas']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Innovation/Innovation.csv')\n",
    "print(list(df))\n",
    "#set the column name to model by\n",
    "col = 'Title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "for index, row in df.iterrows():\n",
    "    if pd.isna(row[col]):\n",
    "        text_data.append([])\n",
    "    else:\n",
    "        tokens = tokenize(row[col])\n",
    "        text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(corpus, open('Innovation/innov_corpus.pkl', 'wb'))\n",
    "dictionary.save('Innovation/innov_dictionary.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the number of topics you wish to retrieve with LDA. Suggested number of topics should be between 3 and 10, as more than that are difficult to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 5\n",
    "ldamodel = LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('Innovation/model5.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the results of topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1. Relevant terms: 0.017*\"sustainable\" + 0.015*\"development\" + 0.012*\"management\" + 0.011*\"based\"\n",
      "Topic 2. Relevant terms: 0.013*\"development\" + 0.008*\"small\" + 0.007*\"chapter\" + 0.006*\"local\"\n",
      "Topic 3. Relevant terms: 0.012*\"chapter\" + 0.011*\"future\" + 0.011*\"economic\" + 0.010*\"review\"\n",
      "Topic 4. Relevant terms: 0.016*\"development\" + 0.014*\"change\" + 0.010*\"climate\" + 0.010*\"education\"\n",
      "Topic 5. Relevant terms: 0.035*\"chapter\" + 0.020*\"management\" + 0.015*\"performance\" + 0.014*\"marketing\"\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(\"Topic {}\".format(int(topic[0]) + 1) + \". Relevant terms: {}\".format(topic[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
